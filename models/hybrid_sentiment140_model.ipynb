{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nives\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nives\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\nives\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nives\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# ---------------------------\n",
    "# Function to Extract Word-Level Sentiment Features\n",
    "# ---------------------------\n",
    "def get_word_sentiment_scores(text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    scores = []\n",
    "    for token in tokens:\n",
    "        sentiment = sia.polarity_scores(token)\n",
    "        scores.append([sentiment['pos'], sentiment['neg'], sentiment['neu'], sentiment['compound']])\n",
    "    return torch.tensor(scores, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Dataset Class with Padded Sentiment Features\n",
    "# ---------------------------\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        # Ensure texts are in list format and of type string\n",
    "        if isinstance(texts, pd.Series):  # Check if texts is a pandas Series\n",
    "            texts = texts.tolist()  # Convert to list\n",
    "        elif not isinstance(texts, list):  # Check if it's not already a list\n",
    "            raise ValueError(\"Expected 'texts' to be a pandas Series or list of strings.\")\n",
    "        \n",
    "        # Ensure that each text is a string\n",
    "        texts = [str(text) for text in texts]\n",
    "\n",
    "        # Debug: Check the type of texts and sample text data\n",
    "        print(f\"Texts type: {type(texts)}\")\n",
    "        print(f\"Sample texts: {texts[:5]}\")  # Print first 5 texts\n",
    "\n",
    "        # Tokenize the texts\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "        # Debug: Check the tokenized output\n",
    "        print(f\"Tokenized encodings: {self.encodings.keys()}\")\n",
    "\n",
    "        # Generate sentiment features for each word (ensure text is in string format)\n",
    "        self.sentiment_features = [\n",
    "            torch.tensor(get_word_sentiment_scores(str(text), tokenizer), dtype=torch.float32) for text in texts\n",
    "        ]\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'sentiment_features': self.sentiment_features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Collate Function for Dataloader\n",
    "# ---------------------------\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "\n",
    "    # Pad sentiment features\n",
    "    sentiment_features = [item['sentiment_features'] for item in batch]\n",
    "    padded_sentiment_features = pad_sequence(sentiment_features, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'sentiment_features': padded_sentiment_features,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Texts type: <class 'list'>\n",
      "Sample texts: ['ya quot like palm pre touchston charger readynow ye sound good beer readi prelaunch', 'felt earthquak afternoon seem epicent', 'ruffl shirt like likey', 'pretti bad night crappi morn fml buttfac didnt say could go work today', 'yeah clear view']\n",
      "Tokenized encodings: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nives\\AppData\\Local\\Temp\\ipykernel_9540\\1069351092.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_word_sentiment_scores(str(text), tokenizer), dtype=torch.float32) for text in texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts type: <class 'list'>\n",
      "Sample texts: ['ahhh hope ok', 'cool tweet app razr 2', 'know famili drama lame hey next time u hang kim n u guy like sleepov whatev ill call u', 'school email open geographi stuff revis stupid school', 'upper airway problem']\n",
      "Tokenized encodings: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Load Preprocessed Data\n",
    "# ---------------------------\n",
    "df = pd.read_csv(\"../cleaned_data/preprocessed_sentiment140.csv\")\n",
    "# cols = ['labels','id','date','query','user','tweet']\n",
    "# df = pd.read_csv(\"C:/Users/nives/OneDrive/Desktop/HybridSA/datasets/sentiment140.csv\",encoding=\"ISO-8859-1\",header=None, names=cols)\n",
    "# df.drop(['id','date', 'query','user'],axis=1,inplace=True)\n",
    "\n",
    "# df.rename({'label': 'labels'}, axis=1, inplace=True)\n",
    "print(df['labels'].unique())\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['tweet'], df['labels'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = IMDBDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Hybrid Model Definition\n",
    "# ---------------------------\n",
    "class HybridSentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridSentimentModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.lexicon_fc = nn.Linear(4, 16)  # 4 lexicon features -> 16 dimensions\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size + 16, 2)  # Combined -> 2 classes\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sentiment_features):\n",
    "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        roberta_cls = roberta_output.last_hidden_state[:, 0, :]  # Fixed: Using CLS token\n",
    "\n",
    "        lexicon_out = self.lexicon_fc(sentiment_features.mean(dim=1))  # Mean word-level features\n",
    "        combined = torch.cat((roberta_cls, lexicon_out), dim=1)\n",
    "        \n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Initialize Model\n",
    "# ---------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridSentimentModel().to(device)\n",
    "\n",
    "# Freeze RoBERTa layers initially\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "total_steps = len(train_loader) * 4  # For 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_model(model, train_loader, val_loader, epochs=4):\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 2:  # Unfreeze RoBERTa after 2 epochs\n",
    "            for param in model.roberta.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_features = batch['sentiment_features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, sentiment_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_features = batch['sentiment_features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, sentiment_features)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# ---------------------------\n",
    "# Train the Model\n",
    "# ---------------------------\n",
    "train_model(model, train_loader, val_loader, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Save the Model\n",
    "# ---------------------------\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'tokenizer': tokenizer\n",
    "}, 'sentiment140model_hybrid.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load VADER Sentiment Analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "checkpoint = torch.load('sentiment140model_hybrid.pth', map_location=torch.device('cpu'))\n",
    "tokenizer = checkpoint['tokenizer']\n",
    "\n",
    "class HybridSentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridSentimentModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.lexicon_fc = nn.Linear(4, 16)  # 4 lexicon features -> 16 dimensions\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size + 16, 2)  # Combined -> 2 classes\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sentiment_features):\n",
    "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        roberta_cls = roberta_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        lexicon_out = self.lexicon_fc(sentiment_features.mean(dim=1))  # Mean word-level features\n",
    "        combined = torch.cat((roberta_cls, lexicon_out), dim=1)\n",
    "        \n",
    "        return self.fc(combined)\n",
    "\n",
    "# Initialize model and load weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridSentimentModel().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Function to extract word sentiment scores\n",
    "def get_word_sentiment_scores(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    scores = [sia.polarity_scores(token) for token in tokens]\n",
    "    return torch.tensor([[s['pos'], s['neg'], s['neu'], s['compound']] for s in scores], dtype=torch.float32)\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "\n",
    "    # Extract word-level sentiment features\n",
    "    sentiment_features = get_word_sentiment_scores(text).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    sentiment_features = sentiment_features.to(device)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask, sentiment_features)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "\n",
    "    # Define class labels\n",
    "    class_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"predicted_class\": class_labels[predicted_class.item()],\n",
    "        \"confidence\": confidence.item() * 100  # Convert to percentage\n",
    "    }\n",
    "\n",
    "# Example inference\n",
    "sample_text = \"I absolutely loved the movie! It was a masterpiece.\"\n",
    "result = predict_sentiment(sample_text)\n",
    "print(\"Input Text:\", result['text'])\n",
    "print(\"Predicted Sentiment:\", result['predicted_class'])\n",
    "print(\"Confidence Score:\", result['confidence']) \n",
    "\n",
    "sample_text3 = \"Oh wow, another software update. I can't wait to lose all my settings again!\"\n",
    "result3 = predict_sentiment(sample_text3)\n",
    "print(\"Input Text:\", result3['text'])\n",
    "print(\"Predicted Sentiment:\", result3['predicted_class'])\n",
    "print(\"Confidence Score:\", result3['confidence']) \n",
    "\n",
    "sample_text2 = \"The ending was so bad that it ruined the entire movie for me.\"\n",
    "result2 = predict_sentiment(sample_text2)\n",
    "print(\"\\nInput Text:\", result2['text'])\n",
    "print(\"Predicted Sentiment:\", result2['predicted_class'])\n",
    "print(\"Confidence Score:\", result2['confidence'])\n",
    "\n",
    "sample_text4 = \"Oh great, my flight got delayed. More time to enjoy these uncomfortable airport chairs!\"\n",
    "result4 = predict_sentiment(sample_text4)\n",
    "print(\"\\nInput Text:\", result4['text'])\n",
    "print(\"Predicted Sentiment:\", result4['predicted_class'])\n",
    "print(\"Confidence Score:\", result4['confidence'])\n",
    "\n",
    "\n",
    "# Handling low-confidence predictions\n",
    "# if result2['confidence'] < 50:\n",
    "#     print(\"This prediction has low confidence and might be unreliable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
